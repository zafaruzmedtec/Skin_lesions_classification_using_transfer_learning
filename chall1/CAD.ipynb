{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CAD.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"slideshow":{"slide_type":"-"},"id":"wNqz411vS6FT","colab_type":"text"},"cell_type":"markdown","source":["# Computer Aided Diagnosis: Diagnosis in dermoscopic images\n","## Valerio Di Sano, Zafar Toshpulatov, Antoine Merlet\n","\n","![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqOdINfzZ4LQu82vL_1PYgMLL8jISvCGTF5fY71zMr01weZ7gGdQ \"UdG\")\n"]},{"metadata":{"id":"cIuXe5x3S6FV","colab_type":"text"},"cell_type":"markdown","source":["### Disclaimer: This Notebook aims to demonstrate our results on Skin Lesion Classification using Deep Learning.\n","### All data (Software, Model weights, Packages) should be installed and organized as stated at the end of this file.\n"]},{"metadata":{"id":"Kvd6mKJWS6FW","colab_type":"text"},"cell_type":"markdown","source":["## 1. Imports"]},{"metadata":{"id":"SNgqtEXCS6FX","colab_type":"code","colab":{}},"cell_type":"code","source":["import argparse\n","import os\n","import shutil\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.distributed as dist\n","import torch.optim\n","import torch.utils.data\n","import torch.utils.data.distributed\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","from tensorboardX import SummaryWriter\n","\n","best_acc = 0 # \"Global\" holding the best accuracy reached\n","writer = SummaryWriter('runs')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"If1RLon7S6Fa","colab_type":"text"},"cell_type":"markdown","source":["## 2. Parameters"]},{"metadata":{"id":"13qn38syS6Fa","colab_type":"code","colab":{}},"cell_type":"code","source":["data_path = './'  # Path to the dataset\n","nb_classes = 2    # Number of classes\n","\n","# Network\n","arch = 'vgg16_bn' # Architecture of the CNN. Use 'vgg16_bn', 'ResNet50' or 'GoogLeNet'\n","epochs = 300      # Number of epochs\n","checkpoint = 'D:\\ProjectSkin\\chall1\\checkpoint.pth.tar'   # Load saved checkpoint\n","start_epoch = 0   # Starting epoch in checkpoint\n","batch_size = 28   # Number of data inputed into CNN at once. Change depending on GPU RAM\n","\n","# Optimizer setting\n","optimizer = 'adam'    # Choice of the optimizer SGD or adam\n","learning_rate = 1e-5 # Initial optimizer Learning Rate (LR)\n","lr_decay_fact = 0.1  # Multiplier for learning rate reduction\n","lr_decay_time = 30    # Number of epochs before learning_rate * lr_decay_fact\n","momentum_SGD = 0.9   # Momentum for SGD\n","weight_d = 1e-5      # Weight decay (L2 penalty)\n","\n","# Run type\n","evaluate = True  # Set to True to skip training and evaluate only\n","pretrained = True # set True to load Pytorch pretrained weights (ImageNet)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zcV8bs9nS6Fd","colab_type":"text"},"cell_type":"markdown","source":["## 3. Tool functions"]},{"metadata":{"id":"CGW1wKF0S6Fe","colab_type":"code","colab":{}},"cell_type":"code","source":["# --------------------------- Compile without reading is fine ----------------------------\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        \n","def adjust_learning_rate(optimizer, epoch):\n","    \"\"\"Decrease the Learning Rate by multiplier on given number of epoch\"\"\"\n","    lr = learning_rate * (lr_decay_fact ** (epoch // lr_decay_time))\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","        \n","def accuracy(output, target, topk=(1,)): # TODOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n","    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n","    batch_size = target.size(0)\n","\n","    _, pred = output.topk(1, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].view(-1).float().sum(0)\n","        res.append(correct_k.mul_(100.0 / batch_size))\n","    return res\n","\n","def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'): # TODO export in folder with all param\n","    torch.save(state, filename)\n","    if is_best: # if new best model, export weights\n","        shutil.copyfile(filename, 'best_model.pth.tar')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TGUQVMCPS6Fg","colab_type":"text"},"cell_type":"markdown","source":["## 4. Train function"]},{"metadata":{"id":"56dZ0l_tS6Fh","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(train_loader, model, criterion, optimizer, epoch):\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    acc = AverageMeter()\n","\n","    # switch to train mode\n","    model.train()\n","\n","    end = time.time()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    \n","    for i, (input, target) in enumerate(train_loader):\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","\n","        target = target.cuda(async=True)\n","        input_var = torch.autograd.Variable(input)\n","        target_var = torch.autograd.Variable(target)\n","\n","        # compute gradient and do  step\n","        optimizer.zero_grad()\n","        \n","        with torch.set_grad_enabled(True):\n","            # compute output\n","            output = model(input_var)\n","            loss = criterion(output, target_var)\n","            \n","            _, preds = torch.max(output, 1)\n","            \n","            loss.backward()\n","            optimizer.step()\n","        \n","        # measure accuracy and record loss\n","        prec1 = accuracy(output.data, target, topk=(1, 1))\n","        losses.update(loss.data[0], input.size(0))\n","        acc.update(prec1[0], input.size(0))\n","\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if i % 10 == 0:\n","            print('Epoch: [{0}][{1}/{2}]\\t'\n","                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                  'Accuracy {acc.val:.3f} ({acc.avg:.3f})\\t'.format(\n","                   epoch, i, len(train_loader), batch_time=batch_time,\n","                   data_time=data_time, loss=losses, acc=acc))\n","            niter = epoch*len(train_loader)+i\n","            writer.add_scalar('Train/Loss', losses.val, niter)\n","            writer.add_scalar('Train/Accuracy', acc.val, niter)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CNvhSAX7S6Fk","colab_type":"text"},"cell_type":"markdown","source":["## 5. Validate Function"]},{"metadata":{"id":"yRkrAwrhS6Fk","colab_type":"code","colab":{}},"cell_type":"code","source":["def validate(val_loader, model, criterion, epoch):\n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","\n","    # switch to evaluate mode\n","    model.eval()\n","\n","    end = time.time()\n","    for i, (input, target) in enumerate(val_loader):\n","        target = target.cuda(async=True)\n","        \n","        with torch.set_grad_enabled(False):\n","            input_var = torch.autograd.Variable(input)\n","            target_var = torch.autograd.Variable(target)\n","    \n","            # compute output\n","            output = model(input_var)\n","            loss = criterion(output, target_var)\n","    \n","            # measure accuracy and record loss\n","            best_acc = accuracy(output.data, target, topk=(1, 1))\n","            losses.update(loss.data[0], input.size(0))\n","            top1.update(best_acc[0], input.size(0))\n","    \n","            # measure elapsed time\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","    \n","            if i % 10 == 0:\n","                print('Validation: [{0}/{1}]\\t'\n","                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                      'Accuracy {top1.val:.3f} ({top1.avg:.3f})\\t'.format(\n","                       i, len(val_loader), batch_time=batch_time, loss=losses,\n","                       top1=top1))\n","    print(' * Accuracy {top1.avg:.3f}'\n","          .format(top1=top1))\n","    writer.add_scalar('Validation/Loss', losses.avg, epoch)\n","    writer.add_scalar('Validation/Accuracy', top1.avg, epoch)\n","\n","    return top1.avg"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y_CDKl1dS6Fn","colab_type":"text"},"cell_type":"markdown","source":["## Test Fucntion"]},{"metadata":{"id":"qkFHOXVwS6Fo","colab_type":"code","colab":{}},"cell_type":"code","source":["def test_all(tets_loader, model, criterion, epoch):\n","    # switch to evaluate mode\n","    model.eval()\n","    out = np.zeros(len(tets_loader.dataset.imgs))\n","    end = time.time()\n","    for i, (input,target) in enumerate(tets_loader):\n","        with torch.set_grad_enabled(False):\n","            input_var = torch.autograd.Variable(input)\n","    \n","            # compute output\n","            output = model(input_var)\n","            output = output.data.cpu().numpy()[0]\n","            out[i] = output.argmax(axis=0)\n","    return out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GcURoMqJS6Fr","colab_type":"text"},"cell_type":"markdown","source":["## 6. Initialize network"]},{"metadata":{"id":"Ir-6WUkeS6Fs","colab_type":"code","colab":{},"outputId":"8da01583-38ad-4ebc-f29a-c178975a4e7d"},"cell_type":"code","source":["# Load architecture\n","if arch == 'vgg16_bn':\n","    print(\"Using VGG16_bn (batch normalization)\")\n","    model = models.vgg16_bn(pretrained=True)    # Load given model with pretrained weigths (download if needed)\n","    num_ftrs = model.classifier[6].in_features  # Get number of output of the second last layer\n","    model.classifier[6] = nn.Linear(num_ftrs,nb_classes)   # Reset last layer weights, change number of output\n","    model.features = torch.nn.DataParallel(model.features) # Needed for local processing\n","    model.cuda() # Transfer model to GPU\n","elif arch == 'ResNet50':\n","    model = models.ResNet50(pretrained=True) # TODOOOOOOOOOOOO\n","    torch.nn.DataParallel(model).cuda() # Transfer model to GPU\n","elif arch == 'GoogLeNet':\n","    model = models.GoogLeNet(pretrained=True) # TODOOOOOOOOOOOO\n","    torch.nn.DataParallel(model).cuda() # Transfer model to GPU\n","else :\n","    print('Error: Unrecognized architecture. Exiting...') "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using VGG16_bn (batch normalization)\n"],"name":"stdout"}]},{"metadata":{"id":"9_4fadzIS6Fw","colab_type":"code","colab":{}},"cell_type":"code","source":["# Define working tools\n","if optimizer == 'SGD' :\n","    optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=momentum_SGD, weight_decay=weight_d)\n","elif optimizer == 'adam':\n","    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=weight_d) # TODOOOOOOOOOOOOOOOOOOOO\n","    \n","criterion = nn.CrossEntropyLoss().cuda()\n","\n","cudnn.benchmark = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_8FxFnBaS6Fz","colab_type":"code","colab":{},"outputId":"81376f1b-285b-4aa3-a5d5-d842830fdb64"},"cell_type":"code","source":["# Load training checkpoint\n","if checkpoint: \n","    if os.path.isfile(checkpoint):\n","        data = torch.load(checkpoint)   # get the file\n","        start_epoch = data['epoch']     # load previous epoch\n","        best_acc = data['best_acc']     # load previous best accuracy\n","        model.load_state_dict(data['state_dict'])    # load previous weights\n","        optimizer.load_state_dict(data['optimizer']) # loadprevious optimiwzer stat\n","        print(\"Loaded checkpoint '{}' (epoch {})\".format(checkpoint, data['epoch']))\n","    else:\n","        print(\"No checkpoint at '{}'\".format(checkpoint))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded checkpoint 'D:\\ProjectSkin\\chall1\\checkpoint.pth.tar' (epoch 112)\n"],"name":"stdout"}]},{"metadata":{"id":"zO_Zq5EPS6F2","colab_type":"text"},"cell_type":"markdown","source":["## 7. Data Loading"]},{"metadata":{"id":"7N_FCFCGS6F2","colab_type":"code","colab":{}},"cell_type":"code","source":["train_dir = os.path.join(data_path, 'train') # Get train data folder\n","\n","train_dataset = datasets.ImageFolder( # Prepare training data\n","    train_dir,\n","    transforms.Compose([\n","        transforms.RandomSizedCrop(224),   # randomly crop images to fit ImageNet input size\n","        transforms.RandomHorizontalFlip(), # data augmentation\n","        transforms.ToTensor(), \n","        transforms.Normalize( # Setup normalization according to ImageNet\n","            mean=[0.485, 0.456, 0.406], \n","            std=[0.229, 0.224, 0.225]),\n","    ]))\n","\n","train_loader = torch.utils.data.DataLoader( # Define loading schem\n","    train_dataset, \n","    batch_size=batch_size, # Number of images per batch\n","    shuffle=True,          # Shuffle data order on each epoch\n","    pin_memory=True)       # Use CUDA pinned memory for tensors"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"ygkdYYKMS6F4","colab_type":"code","colab":{}},"cell_type":"code","source":["val_dir = os.path.join(data_path, 'val') # Get validation data folder\n","\n","val_dataset = datasets.ImageFolder( # Prepare validation data\n","    val_dir, \n","    transforms.Compose([\n","        transforms.Scale(256),\n","        transforms.CenterCrop(224), # Crop to fit input size. ROI assumed at center\n","        transforms.ToTensor(), \n","        transforms.Normalize( # Setup normalization according to ImageNet\n","            mean=[0.485, 0.456, 0.406], \n","            std=[0.229, 0.224, 0.225]),\n","    ]))\n","\n","val_loader = torch.utils.data.DataLoader(\n","    val_dataset,\n","    batch_size=batch_size, \n","    shuffle=False, # Explicitly reminded: do not shuffle for validation (for stats)\n","    pin_memory=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PuF5KP2WS6F6","colab_type":"code","colab":{}},"cell_type":"code","source":["test_dir = os.path.join(data_path,'test') # Get validation data folder\n","\n","test_dataset = datasets.ImageFolder( # Prepare test data\n","    test_dir, \n","    transforms.Compose([\n","        transforms.Scale(256),\n","        transforms.CenterCrop(224), # Crop to fit input size. ROI assumed at center\n","        transforms.ToTensor(), \n","        transforms.Normalize( # Setup normalization according to ImageNet\n","            mean=[0.485, 0.456, 0.406], \n","            std=[0.229, 0.224, 0.225]),\n","    ]))\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset,\n","    batch_size=1, \n","    shuffle=False, # Explicitly reminded: do not shuffle for validation (for stats)\n","    pin_memory=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UTs1dkrES6F9","colab_type":"text"},"cell_type":"markdown","source":["## 8. Train the network"]},{"metadata":{"id":"b7wYQfCyS6F9","colab_type":"code","colab":{}},"cell_type":"code","source":["# --------- Evaluate only -------------\n","if evaluate:\n","    validate(evaluate, model, criterion, 1)\n","# -------------------------------------"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j53-p50dS6GB","colab_type":"code","colab":{}},"cell_type":"code","source":["for epoch in range(start_epoch, epochs):\n","    \n","    adjust_learning_rate(optimizer, epoch)\n","    train(train_loader, model, criterion, optimizer, epoch)\n","    acc = validate(val_loader, model, criterion, epoch)\n","\n","    is_best = acc > best_acc # is new accuracy global best\n","    best_acc = max(acc, best_acc) # update best accuracy if needed\n","    \n","    save_checkpoint({\n","        'epoch': epoch + 1,\n","        'arch': arch,\n","        'state_dict': model.state_dict(),\n","        'best_acc': best_acc,\n","        'optimizer' : optimizer.state_dict(),\n","    }, is_best)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hkTaKxvRS6GE","colab_type":"text"},"cell_type":"markdown","source":["## TESTING"]},{"metadata":{"id":"oytcjS9yS6GF","colab_type":"code","colab":{}},"cell_type":"code","source":["# --------- Evaluate only -------------\n","result = test_all(test_loader, model, criterion, 1)\n","np.savetxt(\"test_1.csv\", result, delimiter=\",\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5_86RhWaS6GH","colab_type":"code","colab":{},"outputId":"b202125b-41aa-47d0-da9b-5e2d86d01b4c"},"cell_type":"code","source":["np.sum((result==0))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["344"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"Z6QPkEzmS6GJ","colab_type":"text"},"cell_type":"markdown","source":["# Requirements"]},{"metadata":{"id":"aVTNkTlQS6GK","colab_type":"text"},"cell_type":"markdown","source":["#### Software and packages:\n","* Python 3.6\n","* CUDA 9.0\n","* cuDNN 7.4.1\n","\n","* Pytorch 0.4.0\n","* Torchvision \n","\n","#### Model weigths:\n","If not given with this Notebook, please download them (1.10GB) here: https://drive.google.com/open?id=14mzlsTjZf4p-ihovbOTjyG_r1dZ2-alb   (not uploaded yet)"]}]}